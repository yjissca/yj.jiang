现在，几乎所有人都承认，将世界的结构化信息和知识结合起来以回答语义丰富的查询是计算机科学的主要挑战之一，并且有可能对整个世界产生巨大影响。
这导致了将近30年的信息集成研究[15,19]，最终是语义Web及其相关技术[1,11,13]。 此类努力通常仅在相对较小且专门的领域中受到关注，在这些领域中，可以商定封闭的本体，词汇或方案。
但是，更广泛的语义Web愿景尚未实现，而此类工作面临的最大挑战之一是如何在系统中获取足够的“有趣”且广泛有用的信息，以使其对普通受众有用和可访问。

一个挑战是，在开发数据之前设计本体或架构的传统“自上而下”模型会在Web规模上崩溃：数据和元数据都必须不断发展，它们必须服务于许多不同的社区。
因此，最近出现了使用增量式和Web 2.0启发式协作方法构建语义Web基层样式的运动[10,12,13]。
这种协作的，基层的语义网需要一种新的结构化信息表示和管理模型：首先，它必须处理不一致，歧义，不确定性，数据来源[3,6,8,7]和隐式知识。 统一的方式。

沿这些方向推动协同研究的最有效方法也许就是提供丰富的各种数据集。 这将使研究人员能够开发，比较和评估不同的提取，推理和不确定性管理技术，并在Web上部署操作系统。

DBpedia项目已经从Wikipedia百科全书中获得了这样的数据语料库。 Wikipedia的访问量很高并且正在不断修订中（例如，根据alexa.com的说法，Wikipedia是2007年第三季度访问量排名第9的网站）。
 Wikipedia版本提供250多种语言版本，其中英语版本占195万以上。 像许多其他Web应用程序一样，Wikipedia的问题在于其搜索功能仅限于全文搜索，这仅允许非常有限地访问此有价值的知识库。
 正如广为宣传的那样，维基百科还展现了协作编辑数据的许多挑战性属性：它具有矛盾的数据，不一致的分类惯例，错误甚至是垃圾邮件。
 
 DBpedia项目专注于将Wikipedia内容转换为结构化知识的任务，以便可以针对它使用语义Web技术-对Wikipedia进行复杂的查询，将其链接到Web上的其他数据集，或者创建新的应用程序或混搭。 我们做
以下贡献：

–我们开发了一个信息提取框架，该框架将Wikipedia内容转换为RDF。 基本组成部分构成了基础，可以在此基础上进一步进行信息提取，聚类，不确定性管理和查询处理的研究。
–我们以大型，多域RDF数据集的形式提供Wikipedia内容，可将其用于各种语义Web应用程序。 DBpedia数据集包含1.03亿个RDF三元组。
–我们将DBpedia数据集与其他开放数据集互连在一起。 这样就形成了一个庞大的数据网络，总共包含约20亿个RDF三元组。
–我们开发了一系列接口和访问模块，以便可以通过Web服务访问数据集并链接到其他站点。

DBpedia数据集可以导入到第三方应用程序中，也可以使用各种DBpedia用户界面在线访问。 
图1概述了DBpedia信息提取过程，并显示了如何将提取的数据发布到Web上。 
这些主要的DBpedia接口当前使用Virtuoso [9]和MySQL作为存储后端。 

本文的结构如下：
在第2节中，我们概述了DBpedia信息提取技术。
在第3节中，介绍了所得数据集。
在第4节中，我们展示了以编程方式访问DBpedia数据集的方法。
在第5节中，我们介绍了我们的方法。 DBpedia数据集如何成为开放数据网络核心的愿景。
在第6节中，我们展示了几个用于访问DBpedia的用户界面，
最后在第7节中，回顾了相关工作。


2从维基百科提取结构化信息
Wikipedia文章主要由自由文本组成，但也包含不同类型的结构化信息，例如信息框模板，分类信息，图像，地理坐标，到外部网页的链接以及跨Wikipedia不同语言版本的链接。
Mediawiki4是用于运行Wikipedia的软件。由于此Wiki系统的性质，基本上所有的编辑，链接，元数据注释都通过添加特殊的语法构造在文章文本内完成。
因此，可以通过解析这些语法构造的文章文本来获得结构化信息。由于MediaWiki本身利用某些信息来呈现用户界面，因此某些信息被缓存在关系数据库表中。
定期将不同Wikipedia语言版本的关键关系数据库表（包括包含文章文本的表）的转储定期发布在Web上5。
基于这些数据库转储，我们当前使用两种不同的方法来提取语义关系：
（1）我们将关系数据库表中已经存储的关系映射到RDF上；（2）我们直接从文章文本和信息框模板中提取其他信息在文章中。

我们用Wikipedia信息框模板示例说明了从文章文本中提取语义的方法。图2显示了信息框模板（在Wikipedia文章中编码）和南韩小镇釜山的渲染输出。
信息框提取算法检测到此类模板，并使用模式匹配技术识别其结构。它选择有效模板，然后将其解析并转换为RDF三元组。该算法使用后处理技术来提高提取质量。
识别MediaWiki链接并将其转换为合适的URI，检测公共单元并将其转换为数据类型。此外，该算法可以检测对象列表，然后将其转换为RDF列表。
有关信息框提取算法的详细信息（包括数据类型识别，清理启发式方法和标识符生成之类的问题）可以在[2]中找到。所有提取算法均使用PHP实施，并在开放源代码许可下可用6。

3 DBpedia数据集
DBpedia数据集目前提供有关195万多个“事物”的信息，
包括至少80,000人，70,000个场所，35,000个音乐专辑，12,000 lms。 它包含657,000个图像链接，1,600,000个相关外部网页链接，180,000个其他RDF数据集外部链接，207,000个Wikipedia类别和75,000个YAGO类别[16]。
 用13种不同语言的简短摘要描述了DBpedia的概念。
 这些摘要摘自维基百科的英语，德语，法语，西班牙语，意大利语，葡萄牙语，波兰语，瑞典语，荷兰语，日语，中文，俄语，芬兰语和挪威语版本。
 DBpedia数据集总共包含约1.03亿个RDF三元组。 该数据集作为一组较小的RDF文件供下载。 表1概述了这些文件。
 
 某些数据集（例如Persons或Infoboxes数据集）在语义上很丰富，因为它们包含非常特定的信息。其他（例如PageLinks数据集）包含没有特定语义的元数据（例如文章之间的链接）。
 但是，后者可能是有益的，例如用于得出概念之间的紧密程度或搜索结果的相关性。 DBpedia数据集中描述的195万个资源中的每一个都由URI引用标识，其格式为http://dbpedia.org/resource/Name，
其中，名称是从源Wikipedia文章的URL提取的，其形式为http://en.wikipedia.org/wiki/Name。因此，每个资源都直接与英语维基百科文章联系在一起。这为DBpedia标识符带来了某些有益的特性：
{它们涵盖了广泛的百科全书主题，
{它们是由社区共识定义的，
{有明确的管理政策，
{并且，可以在知名的网站（Wikipedia页面）上找到该概念的详尽文本定义。