

七章 贝叶斯分类器

我们希望得到一个分类方法（判定准则）h，使得这个判定准则对每一个样本，预测错的期望损失最小。那么这个h就叫做贝叶斯最优分类器 。这时总体的期望损失（风险）称为贝叶斯风险。

1）贝叶斯分类器是基于贝叶斯决策论的基础上构建的，目的是对每个样本 x，选择能使后验概率 P( c | x )最大的类别标记


2）现实中?P( c | x ) 较难获得，所以将求后验概率 P( c | x ) 的问题转变为求先验概率 P( c ) 和条件概率 P( x | c )


3）根据大数定律，P( c ) 可通过各类样本出现的频率来进行估计


4）在完全遵守“属性条件独立性假设”时，类条件概率 P( x | c ) 可通过每个属性单独的条件概率的乘积得到


5）在适当放松“属性条件独立性假设”时，类条件概率 P( x | c ) 可通过每个属性在其父结点下的条件概率的乘积得到


6）在不清楚属性的条件独立性情况时，贝叶斯网是一个有效的算法，能对属性之间的依赖关系进行计算和评估，以找出一个最佳的分类模型

7）贝叶斯网学习的首要任务就是根据训练数据集来找出结构最“恰当”的贝叶斯网结构


8）“平滑”处理是解决零这个特殊值对结果不利影响的一个好策略


9）当遇到不完整的训练样本时，可通过使用EM算法对模型参数进行估计来解决


十章 降维：

一般的，将特征量从n维降到k维：

以最近重构性为目标，PCA的目标是找到k个向量，将所有样本投影到这k个向量构成的超平面，使得投影的距离最小（或者说投影误差projection error最小）。

以最大可分性为目标，PCA的目标是找到k个向量，将所有样本投影到这k个向量构成的超平面，使得样本点的投影能够尽可能的分开，也就是使投影后的样本点方差最大化。

1）?k 近邻学习是简单常用的分类算法，在样本分布充足时，其最差误差不超过贝叶斯最优分类器的两倍

2）实际情况下由于属性维度过大，会导致“维数灾难”，这是所有机器学习中的共同障碍

3）缓解维数灾难的有效途径是降维，即将高维样本映射到低维空间中，这样不仅属性维度降低减少了计算开销，还增大了样本密度

4）降维过程中必定会对原始数据的信息有所丢失，所以根据不同的降维目标我们可以得到不同的降维方法

5）多维缩放的目标是要保证降维后样本之间的距离不变

6）线性降维方法目标是要保证降维到的超平面能更好的表示原始数据

7）核线性降维方法目标是通过核函数和核方法来避免采样空间投影到高维空间再降维之后的低维结构丢失

8）等度量映射的目标是让样本在“流形”上的距离在降维之后仍能保持

9）局部线性嵌入的目标是让样本由其邻域向量重构关系在降维后仍能保持

10）度量学习绕过降维的过程，将学习目标转化为对距离度量计算的权重矩阵的学习


十一章 特征选择：


1）特征选择和降维计算是处理高维数据的两大主流技术


2）特征选择涉及两个关键环节：如何根据反馈来获取下一个特征子集（子集搜索 subset search）？如何评价候选特征子集的好坏（子集评价 subset evaluation）


3）子集搜索是通过贪婪算法每次以一定的策略对特征子集进行添加或删除以使得新的子集能对性能进行提升。当没有更好的选择能使性能提升时，停止子集搜索过程


4）子集评价的本质是比较特征子集 A 对数据集的划分与实际标记信息 Y 对数据集的划分的差别，其它能判断两个划分差异的机制都能用于特征子集评价


5）不同的特征选择方法实际上是显式或隐式的结合了某种或多种子集搜索和子集评价机制


6）过滤式方法先对数据集进行特征选择，然后再训练学习器，特征选择过程与后续学习器无关；即先对特征进行“过滤”，然后用过滤后的特征来训练模型


7）包裹式选择直接把最终将要使用的学习器性能作为特征子集的评价标准；根据学习器选择最有利于性能、“量身打造”的特征子集


8）从学习性能上来看，包裹式特征选择比过滤式特征选择好许多，但是计算开销前者要比后者大得多


9）这种为普通稠密表达的样本找到合适的字典，将样本转化为稀疏表示形式，从而使学习任务得以简化，模型复杂度得以降低，通常称为“字典学习”（dictionary learning），亦称“稀疏编码”（sparse coding）


10）压缩感知的核心思想是：将原始数据通过字典学习表示成稀疏表示时，可以比较好的利用其稀疏性复原原始数据


十二章 计算学习理论：

1）机器学习理论研究的是关于通过“计算”来进行“学习”的理论，即关于机器学习的理论基础，其目的是分析学习任务的困难本质，为学习算法提供理论保证，并根据分析结果指导算法设计

2）机器学习理论研究的一个关键是研究算法对应的假设空间是否是可学习的

3）对于具体的假设空间，其可学习性是指该假设空间是否满足其泛化误差小于误差参数的概率在置信空间内

3）通过分析不同情况下假设空间的泛化误差界的范围，可以了解该假设空间是否可学习

4）对于有限假设空间，可以根据 PAC 学习理论来分析假设空间的可学习性

5）对于无限假设空间，我们通过 VC 维分析来度量假设空间的复杂度，并可知任何 VC 维有限的假设空间 H 都是（不可知）PAC可学习的

6）基于 VC 维的泛化误差界是分布无关（distribution-free）、数据独立（data-independent）的

7）Rademacher复杂度在 VC 维的基础上考虑了数据样本分布D

8）在一个确定的训练集上，经验 Rademacher 复杂度其实计算的是假设空间 H 与随机噪音相关性的期望，这个值越大，则说明假设空间与随机噪音拟合地越好，也说明这个假设空间越复杂

9）假设训练集样本采样自分布D，则?Rademacher 复杂度是分布D上的经验 Rademacher 复杂度的期望

10）稳定性分析是希望基于具体学习算法的设计来考虑学习问题本身的性质

11）算法“稳定性”考察的是算法在输入发生变化时，输出是否会随之发生较大的变化

12）若学习算法 A 是ERM且稳定的，则假设空间 H 可学习



十三章 半监督学习：

1）半监督学习是相对于监督学习和非监督学习来说的，前者是在标注好了的训练集上训练学习器，并用训练好的学习器去对新的样本进行预测；后者是在未标注的数据集上根据数据本身的分布情况来对数据进行分类

2）所以对于监督学习，让学习器不依赖外界交互、自动地利用为标记样本来提升学习性能，就是半监督学习

3）对于非监督学习，让学习器引入额外的监督信息来获得更好的性能，也是半监督学习

4）半监督学习要利用未标记样本，必然要做一些将未标记样本所揭示的数据分布信息与类别标记想联系的假设，其本质是“相似的样本拥有相似的输出”

5）基于这个假设，我们可以从不同的角度来使用非标记样本的这个特性，这就形成了不同的半监督学习算法


十五章 规则学习：

自顶向下：
从比较一般的规则开始，逐渐添加新文字以缩小规则覆盖范围，直到满足预定条件为止；也称为生成-测试，是规则逐渐特化的过程；更容易产生泛化性能较好的规则，对噪声的鲁棒性比后者强得多，常用于命名规则。

自底向上：
从比较特殊的规则开始，逐渐删除文字以扩散规则覆盖范围，直到满足条件为止。亦称数据驱动方法，是规则逐渐泛化的过程；更适合训练样本较少的情形。常用于一阶规则学习。

剪枝优化的目的是缓解过拟合风险，方法有预剪枝，后剪枝。