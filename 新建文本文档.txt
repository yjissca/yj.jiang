
分布式账本
非对称加密机制
单链区块链  

并不满足企业内部复杂的访问控制要求
提出一种应用区块链的数据访问控制与共享模型 



访问控制树
默克尔树

模型分为 5 层

平行区块链 
区块链服务层:分为企业链(company blockchain,简称CBC)和行业链(industry blockchain,简称IBC).其
中,CBC 记录企业内部的数据存储地址与变化情况,由企业内部节点共同维护,确保不同节点状态一致,
提供基于属性的访问控制服务;IBC 记录行业内部企业之间的数据交换与调用,数据的请求与共享都
将记录在IBC 上以便查询和监管;
类似于分布式系统中的混合制体系结构 

属性基加密
• 链上代码层:提供智能合约服务.主要功能是提供属性基访问控制,即在CBC 与IBC 上提供自定义的访
问控制策略,只有满足特定属性(或级别)的账户才能读取(或写入)数据;


整个行业区块链系统由3 种节点维护,分别为企业节点、行业节点与边缘节点,节点之间关系如图3 所示.
各自职能如下.
(1) 企业节点:用于维护企业链.当新数据达到阈值后,企业节点将数据经过对称加密后存放至底层数据
库,并将其存放地址和密钥及Merkle 树一同放到链上用于查询与验证;
(2) 行业节点:用于维护行业链而非企业内部数据,由行业协会或行业内所有企业共同维护,用于确认行
业内各企业之间的数据交互(数据交集查询).行业节点仅维护行业链而不属于任何企业链.其主要功
能是便于监管机构或行业协会对行业链进行监管;
王秀利等:应用区块链的数据访问控制与共享模型1665
(3) 边缘节点:同时加入企业内部区块链网络与行业区块链网络,用于连接企业链与行业链,进行数据传
递.企业链与行业链使用同一套属性基加密算法,将授权中心CA 部署在边缘节点可以提高资源利用
率.当某企业在行业链上发出数据共享请求时,其他企业边缘节点上的智能合约自动验证其访问权
限,并进行交集操作.


(1) 企业区块链
(2) 行业区块链



3.2.1 企业内部访问控制
3.2.2 行业内部访问控制





项目背景         TV初始化之后，需要有一个向导流程帮助用户设置自己的电视。
功能介绍		 按照流程帮助用户配置电视的多项功能以及设置，如语言，国家，时钟，语音交互，条款政策等等。
你做的部分       APP底层框架的维护，新页面的开发。
遇到的问题		 向导的场景过多，需要适应不同的场景;向导包含的页面多而杂。
解决办法		 根据不同的场景制定不同的json文件，APP launch时只需要读取对应的json文件就可以区分不同的场景。
				 将所有页面划分成多个UI Gadget,每个UG负责创建一部分页面。App只需要创建对应的UG即可。
				 
				 
项目背景         TV可以连接众多的外部设备，用户可能不太清楚如何连接这些外设。
功能介绍		 分门别类的向用户介绍如何连接某一设备，如蓝牙设备，HDMI设备等等。
你做的部分       APP底层框架的维护，新item的开发。
遇到的问题		 同一场景的介绍图片会因为场景不同而变化，所以管理着众多的图片资源，占据了过多的储存空间。
解决办法		 根据产品的种类，将对应会用到的图片资源打包到不同的包中，对于不同的产品，安装不同的包已达到节省存储空间的目的。

基于区块链的软件交付过程管理系统的设计与实现
基于区块链的干部人事系统研究与实现
基于区块链的后勤绩效考核系统的研究与实现
基于区块链的教学信息系统设计与研究
基于区块链的考勤记录系统的研究与实现
基于区块链技术的学分认证系统研究

七章 贝叶斯分类器

我们希望得到一个分类方法（判定准则）h，使得这个判定准则对每一个样本，预测错的期望损失最小。那么这个h就叫做贝叶斯最优分类器 。这时总体的期望损失（风险）称为贝叶斯风险。

1）贝叶斯分类器是基于贝叶斯决策论的基础上构建的，目的是对每个样本 x，选择能使后验概率 P( c | x )最大的类别标记


2）现实中?P( c | x ) 较难获得，所以将求后验概率 P( c | x ) 的问题转变为求先验概率 P( c ) 和条件概率 P( x | c )


3）根据大数定律，P( c ) 可通过各类样本出现的频率来进行估计


4）在完全遵守“属性条件独立性假设”时，类条件概率 P( x | c ) 可通过每个属性单独的条件概率的乘积得到


5）在适当放松“属性条件独立性假设”时，类条件概率 P( x | c ) 可通过每个属性在其父结点下的条件概率的乘积得到


6）在不清楚属性的条件独立性情况时，贝叶斯网是一个有效的算法，能对属性之间的依赖关系进行计算和评估，以找出一个最佳的分类模型

7）贝叶斯网学习的首要任务就是根据训练数据集来找出结构最“恰当”的贝叶斯网结构


8）“平滑”处理是解决零这个特殊值对结果不利影响的一个好策略


9）当遇到不完整的训练样本时，可通过使用EM算法对模型参数进行估计来解决


十章 降维：

一般的，将特征量从n维降到k维：

以最近重构性为目标，PCA的目标是找到k个向量，将所有样本投影到这k个向量构成的超平面，使得投影的距离最小（或者说投影误差projection error最小）。

以最大可分性为目标，PCA的目标是找到k个向量，将所有样本投影到这k个向量构成的超平面，使得样本点的投影能够尽可能的分开，也就是使投影后的样本点方差最大化。

1）?k 近邻学习是简单常用的分类算法，在样本分布充足时，其最差误差不超过贝叶斯最优分类器的两倍

2）实际情况下由于属性维度过大，会导致“维数灾难”，这是所有机器学习中的共同障碍

3）缓解维数灾难的有效途径是降维，即将高维样本映射到低维空间中，这样不仅属性维度降低减少了计算开销，还增大了样本密度

4）降维过程中必定会对原始数据的信息有所丢失，所以根据不同的降维目标我们可以得到不同的降维方法

5）多维缩放的目标是要保证降维后样本之间的距离不变

6）线性降维方法目标是要保证降维到的超平面能更好的表示原始数据

7）核线性降维方法目标是通过核函数和核方法来避免采样空间投影到高维空间再降维之后的低维结构丢失

8）等度量映射的目标是让样本在“流形”上的距离在降维之后仍能保持

9）局部线性嵌入的目标是让样本由其邻域向量重构关系在降维后仍能保持

10）度量学习绕过降维的过程，将学习目标转化为对距离度量计算的权重矩阵的学习


十一章 特征选择：


1）特征选择和降维计算是处理高维数据的两大主流技术


2）特征选择涉及两个关键环节：如何根据反馈来获取下一个特征子集（子集搜索 subset search）？如何评价候选特征子集的好坏（子集评价 subset evaluation）


3）子集搜索是通过贪婪算法每次以一定的策略对特征子集进行添加或删除以使得新的子集能对性能进行提升。当没有更好的选择能使性能提升时，停止子集搜索过程


4）子集评价的本质是比较特征子集 A 对数据集的划分与实际标记信息 Y 对数据集的划分的差别，其它能判断两个划分差异的机制都能用于特征子集评价


5）不同的特征选择方法实际上是显式或隐式的结合了某种或多种子集搜索和子集评价机制


6）过滤式方法先对数据集进行特征选择，然后再训练学习器，特征选择过程与后续学习器无关；即先对特征进行“过滤”，然后用过滤后的特征来训练模型


7）包裹式选择直接把最终将要使用的学习器性能作为特征子集的评价标准；根据学习器选择最有利于性能、“量身打造”的特征子集


8）从学习性能上来看，包裹式特征选择比过滤式特征选择好许多，但是计算开销前者要比后者大得多


9）这种为普通稠密表达的样本找到合适的字典，将样本转化为稀疏表示形式，从而使学习任务得以简化，模型复杂度得以降低，通常称为“字典学习”（dictionary learning），亦称“稀疏编码”（sparse coding）


10）压缩感知的核心思想是：将原始数据通过字典学习表示成稀疏表示时，可以比较好的利用其稀疏性复原原始数据


十二章 计算学习理论：

1）机器学习理论研究的是关于通过“计算”来进行“学习”的理论，即关于机器学习的理论基础，其目的是分析学习任务的困难本质，为学习算法提供理论保证，并根据分析结果指导算法设计

2）机器学习理论研究的一个关键是研究算法对应的假设空间是否是可学习的

3）对于具体的假设空间，其可学习性是指该假设空间是否满足其泛化误差小于误差参数的概率在置信空间内

3）通过分析不同情况下假设空间的泛化误差界的范围，可以了解该假设空间是否可学习

4）对于有限假设空间，可以根据 PAC 学习理论来分析假设空间的可学习性

5）对于无限假设空间，我们通过 VC 维分析来度量假设空间的复杂度，并可知任何 VC 维有限的假设空间 H 都是（不可知）PAC可学习的

6）基于 VC 维的泛化误差界是分布无关（distribution-free）、数据独立（data-independent）的

7）Rademacher复杂度在 VC 维的基础上考虑了数据样本分布D

8）在一个确定的训练集上，经验 Rademacher 复杂度其实计算的是假设空间 H 与随机噪音相关性的期望，这个值越大，则说明假设空间与随机噪音拟合地越好，也说明这个假设空间越复杂

9）假设训练集样本采样自分布D，则?Rademacher 复杂度是分布D上的经验 Rademacher 复杂度的期望

10）稳定性分析是希望基于具体学习算法的设计来考虑学习问题本身的性质

11）算法“稳定性”考察的是算法在输入发生变化时，输出是否会随之发生较大的变化

12）若学习算法 A 是ERM且稳定的，则假设空间 H 可学习



十三章 半监督学习：

1）半监督学习是相对于监督学习和非监督学习来说的，前者是在标注好了的训练集上训练学习器，并用训练好的学习器去对新的样本进行预测；后者是在未标注的数据集上根据数据本身的分布情况来对数据进行分类

2）所以对于监督学习，让学习器不依赖外界交互、自动地利用为标记样本来提升学习性能，就是半监督学习

3）对于非监督学习，让学习器引入额外的监督信息来获得更好的性能，也是半监督学习

4）半监督学习要利用未标记样本，必然要做一些将未标记样本所揭示的数据分布信息与类别标记想联系的假设，其本质是“相似的样本拥有相似的输出”

5）基于这个假设，我们可以从不同的角度来使用非标记样本的这个特性，这就形成了不同的半监督学习算法


十五章 规则学习：

自顶向下：
从比较一般的规则开始，逐渐添加新文字以缩小规则覆盖范围，直到满足预定条件为止；也称为生成-测试，是规则逐渐特化的过程；更容易产生泛化性能较好的规则，对噪声的鲁棒性比后者强得多，常用于命名规则。

自底向上：
从比较特殊的规则开始，逐渐删除文字以扩散规则覆盖范围，直到满足条件为止。亦称数据驱动方法，是规则逐渐泛化的过程；更适合训练样本较少的情形。常用于一阶规则学习。

剪枝优化的目的是缓解过拟合风险，方法有预剪枝，后剪枝。
